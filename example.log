2024-10-04 17:08:18,311  * Debugger is active!
2024-10-04 17:08:18,311  * Debugger PIN: 701-207-519
2024-10-04 17:08:22,798 ----------------- STARTING APP
2024-10-04 17:08:22,801 127.0.0.1 - - [04/Oct/2024 17:08:22] "GET / HTTP/1.1" 200 -
2024-10-04 17:09:08,812 ----------------- STARTING APP
2024-10-04 17:09:08,813 127.0.0.1 - - [04/Oct/2024 17:09:08] "GET / HTTP/1.1" 200 -
2024-10-04 17:09:12,192 Prompting llm API with: {'model': 'llama3.2', 'prompt': 'afpoadasd'}
2024-10-04 17:09:12,199 Starting new HTTP connection (1): localhost:11434
2024-10-04 17:09:15,936 http://localhost:11434 "POST /api/generate HTTP/1.1" 200 None
2024-10-04 17:09:15,937 Response status code: 200
2024-10-04 17:09:15,937 127.0.0.1 - - [04/Oct/2024 17:09:15] "POST /answer HTTP/1.1" 200 -
2024-10-04 17:09:36,157 ----------------- STARTING APP
2024-10-04 17:09:36,159 127.0.0.1 - - [04/Oct/2024 17:09:36] "GET / HTTP/1.1" 200 -
2024-10-04 17:09:37,981 Prompting llm API with: {'model': 'llama3.2', 'prompt': 'afpoadasd'}
2024-10-04 17:09:37,984 Starting new HTTP connection (1): localhost:11434
2024-10-04 17:09:38,211 http://localhost:11434 "POST /api/generate HTTP/1.1" 200 None
2024-10-04 17:09:38,211 Response status code: 200
2024-10-04 17:09:38,211 127.0.0.1 - - [04/Oct/2024 17:09:38] "POST /answer HTTP/1.1" 200 -
2024-10-04 17:09:48,257 Prompting llm API with: {'model': 'llama3.2', 'prompt': 'ölamölaölsälf'}
2024-10-04 17:09:48,261 Starting new HTTP connection (1): localhost:11434
2024-10-04 17:09:48,995 http://localhost:11434 "POST /api/generate HTTP/1.1" 200 None
2024-10-04 17:09:48,995 Response status code: 200
2024-10-04 17:09:48,996 127.0.0.1 - - [04/Oct/2024 17:09:48] "POST /answer HTTP/1.1" 200 -
2024-10-04 17:11:59,170 ----------------- STARTING APP
2024-10-04 17:11:59,172 127.0.0.1 - - [04/Oct/2024 17:11:59] "GET / HTTP/1.1" 200 -
2024-10-04 17:12:05,837 ----------------- STARTING APP
2024-10-04 17:12:05,838 127.0.0.1 - - [04/Oct/2024 17:12:05] "GET / HTTP/1.1" 200 -
2024-10-04 17:12:09,538 Prompting llm API with: {'model': 'llama3.2', 'prompt': ',skasd'}
2024-10-04 17:12:09,541 Starting new HTTP connection (1): localhost:11434
2024-10-04 17:12:10,037 http://localhost:11434 "POST /api/generate HTTP/1.1" 200 None
2024-10-04 17:12:10,037 Response status code: 200
2024-10-04 17:12:10,038 127.0.0.1 - - [04/Oct/2024 17:12:10] "POST /answer HTTP/1.1" 200 -
2024-10-04 17:13:32,663 ----------------- STARTING APP
2024-10-04 17:13:32,664 127.0.0.1 - - [04/Oct/2024 17:13:32] "GET / HTTP/1.1" 200 -
2024-10-04 17:13:51,272 ----------------- STARTING APP
2024-10-04 17:13:51,273 127.0.0.1 - - [04/Oct/2024 17:13:51] "GET / HTTP/1.1" 200 -
2024-10-04 17:14:01,649 ----------------- STARTING APP
2024-10-04 17:14:01,650 127.0.0.1 - - [04/Oct/2024 17:14:01] "GET / HTTP/1.1" 200 -
2024-10-04 17:14:23,563 ----------------- STARTING APP
2024-10-04 17:14:23,564 127.0.0.1 - - [04/Oct/2024 17:14:23] "GET / HTTP/1.1" 200 -
2024-10-04 17:14:30,863 ----------------- STARTING APP
2024-10-04 17:14:30,864 127.0.0.1 - - [04/Oct/2024 17:14:30] "GET / HTTP/1.1" 200 -
2024-10-04 17:14:37,088 ----------------- STARTING APP
2024-10-04 17:14:37,089 127.0.0.1 - - [04/Oct/2024 17:14:37] "GET / HTTP/1.1" 200 -
2024-10-04 17:14:43,440 ----------------- STARTING APP
2024-10-04 17:14:43,441 127.0.0.1 - - [04/Oct/2024 17:14:43] "GET / HTTP/1.1" 200 -
2024-10-04 17:15:54,727 Prompting llm API with: {'model': 'llama3.2', 'prompt': 'akdlkasldk'}
2024-10-04 17:15:54,732 Starting new HTTP connection (1): localhost:11434
2024-10-04 17:15:55,302 http://localhost:11434 "POST /api/generate HTTP/1.1" 200 None
2024-10-04 17:15:55,302 Response status code: 200
2024-10-04 17:15:55,303 127.0.0.1 - - [04/Oct/2024 17:15:55] "POST /answer HTTP/1.1" 200 -
2024-10-04 17:16:11,114 Prompting llm API with: {'model': 'llama3.2', 'prompt': 'choose topic'}
2024-10-04 17:16:11,117 Starting new HTTP connection (1): localhost:11434
2024-10-04 17:16:11,560 http://localhost:11434 "POST /api/generate HTTP/1.1" 200 None
2024-10-04 17:16:11,561 Response status code: 200
2024-10-04 17:16:11,561 127.0.0.1 - - [04/Oct/2024 17:16:11] "POST /answer HTTP/1.1" 200 -
2024-10-04 17:16:26,681 Prompting llm API with: {'model': 'llama3.2', 'prompt': 'topic 10'}
2024-10-04 17:16:26,684 Starting new HTTP connection (1): localhost:11434
2024-10-04 17:16:27,191 http://localhost:11434 "POST /api/generate HTTP/1.1" 200 None
2024-10-04 17:16:27,191 Response status code: 200
2024-10-04 17:16:27,192 127.0.0.1 - - [04/Oct/2024 17:16:27] "POST /answer HTTP/1.1" 200 -
2024-10-04 17:39:51,678 ----------------- STARTING APP
2024-10-04 17:39:51,679 127.0.0.1 - - [04/Oct/2024 17:39:51] "GET / HTTP/1.1" 200 -
2024-10-04 17:39:54,501 Prompting llm API with: {'model': 'llama3.2', 'prompt': 'topic 10'}
2024-10-04 17:39:54,505 Starting new HTTP connection (1): localhost:11434
2024-10-04 17:39:58,153 http://localhost:11434 "POST /api/generate HTTP/1.1" 200 None
2024-10-04 17:39:58,154 Response status code: 200
2024-10-04 17:39:58,154 127.0.0.1 - - [04/Oct/2024 17:39:58] "POST /answer HTTP/1.1" 200 -
2024-10-04 17:41:00,637 ----------------- STARTING APP
2024-10-04 17:41:00,638 127.0.0.1 - - [04/Oct/2024 17:41:00] "GET / HTTP/1.1" 200 -
2024-10-04 17:41:03,459 Prompting llm API with: {'model': 'llama3.2', 'prompt': 'asdmkad'}
2024-10-04 17:41:03,461 Starting new HTTP connection (1): localhost:11434
2024-10-04 17:41:04,005 http://localhost:11434 "POST /api/generate HTTP/1.1" 200 None
2024-10-04 17:41:04,006 Response status code: 200
2024-10-04 17:41:04,006 127.0.0.1 - - [04/Oct/2024 17:41:04] "POST /answer HTTP/1.1" 200 -
2024-10-04 17:42:04,521 ----------------- STARTING APP
2024-10-04 17:42:04,522 127.0.0.1 - - [04/Oct/2024 17:42:04] "GET / HTTP/1.1" 200 -
2024-10-04 17:42:09,749 Prompting llm API with: {'model': 'llama3.2', 'prompt': 'lkasclksamd'}
2024-10-04 17:42:09,753 Starting new HTTP connection (1): localhost:11434
2024-10-04 17:42:10,379 http://localhost:11434 "POST /api/generate HTTP/1.1" 200 None
2024-10-04 17:42:10,380 Response status code: 200
2024-10-04 17:42:10,380 127.0.0.1 - - [04/Oct/2024 17:42:10] "POST /answer HTTP/1.1" 200 -
2024-10-04 17:42:18,183 Prompting llm API with: {'model': 'llama3.2', 'prompt': 'blabla'}
2024-10-04 17:42:18,187 Starting new HTTP connection (1): localhost:11434
2024-10-04 17:42:18,669 http://localhost:11434 "POST /api/generate HTTP/1.1" 200 None
2024-10-04 17:42:18,670 Response status code: 200
2024-10-04 17:42:18,671 127.0.0.1 - - [04/Oct/2024 17:42:18] "POST /answer HTTP/1.1" 200 -
2024-10-04 17:42:56,143 ----------------- STARTING APP
2024-10-04 17:42:56,145 127.0.0.1 - - [04/Oct/2024 17:42:56] "GET / HTTP/1.1" 200 -
2024-10-04 17:42:59,093 Prompting llm API with: {'model': 'llama3.2', 'prompt': 'laslkas'}
2024-10-04 17:42:59,096 Starting new HTTP connection (1): localhost:11434
2024-10-04 17:42:59,586 http://localhost:11434 "POST /api/generate HTTP/1.1" 200 None
2024-10-04 17:42:59,586 Response status code: 200
2024-10-04 17:42:59,587 127.0.0.1 - - [04/Oct/2024 17:42:59] "POST /answer HTTP/1.1" 200 -
2024-10-04 17:43:13,432 ----------------- STARTING APP
2024-10-04 17:43:13,433 127.0.0.1 - - [04/Oct/2024 17:43:13] "GET / HTTP/1.1" 200 -
2024-10-04 17:43:16,619 Prompting llm API with: {'model': 'llama3.2', 'prompt': '.,d ÖD'}
2024-10-04 17:43:16,620 Starting new HTTP connection (1): localhost:11434
2024-10-04 17:43:17,128 http://localhost:11434 "POST /api/generate HTTP/1.1" 200 None
2024-10-04 17:43:17,128 Response status code: 200
2024-10-04 17:43:17,129 127.0.0.1 - - [04/Oct/2024 17:43:17] "POST /answer HTTP/1.1" 200 -
2024-10-04 17:45:57,889 Prompting llm API with: {'model': 'llama3.2', 'prompt': 'explain css in 100 tokens\n'}
2024-10-04 17:45:57,892 Starting new HTTP connection (1): localhost:11434
2024-10-04 17:45:58,554 http://localhost:11434 "POST /api/generate HTTP/1.1" 200 None
2024-10-04 17:45:58,554 Response status code: 200
2024-10-04 17:45:58,554 127.0.0.1 - - [04/Oct/2024 17:45:58] "POST /answer HTTP/1.1" 200 -
2024-10-04 17:47:09,349 ----------------- STARTING APP
2024-10-04 17:47:09,351 127.0.0.1 - - [04/Oct/2024 17:47:09] "GET / HTTP/1.1" 200 -
2024-10-04 17:47:37,373 ----------------- STARTING APP
2024-10-04 17:47:37,375 127.0.0.1 - - [04/Oct/2024 17:47:37] "GET / HTTP/1.1" 200 -
2024-10-04 17:47:56,582 ----------------- STARTING APP
2024-10-04 17:47:56,584 127.0.0.1 - - [04/Oct/2024 17:47:56] "GET / HTTP/1.1" 200 -
2024-10-04 17:47:59,972 Prompting llm API with: {'model': 'llama3.2', 'prompt': 'lk<jdsfMÖDS'}
2024-10-04 17:47:59,976 Starting new HTTP connection (1): localhost:11434
2024-10-04 17:48:00,643 http://localhost:11434 "POST /api/generate HTTP/1.1" 200 None
2024-10-04 17:48:00,643 Response status code: 200
2024-10-04 17:48:00,644 127.0.0.1 - - [04/Oct/2024 17:48:00] "POST /answer HTTP/1.1" 200 -
2024-10-04 17:48:26,078 Prompting llm API with: {'model': 'llama3.2', 'prompt': 'WHY WAS YOUR LAST ANSWER IN GERMAN'}
2024-10-04 17:48:26,080 Starting new HTTP connection (1): localhost:11434
2024-10-04 17:48:26,873 http://localhost:11434 "POST /api/generate HTTP/1.1" 200 None
2024-10-04 17:48:26,873 Response status code: 200
2024-10-04 17:48:26,873 127.0.0.1 - - [04/Oct/2024 17:48:26] "POST /answer HTTP/1.1" 200 -
2024-10-04 17:50:03,029 Prompting llm API with: {'model': 'llama3.2', 'prompt': 'talk about the benefits of docker'}
2024-10-04 17:50:03,031 Starting new HTTP connection (1): localhost:11434
2024-10-04 17:50:03,650 http://localhost:11434 "POST /api/generate HTTP/1.1" 200 None
2024-10-04 17:50:03,650 Response status code: 200
2024-10-04 17:50:03,651 127.0.0.1 - - [04/Oct/2024 17:50:03] "POST /answer HTTP/1.1" 200 -
